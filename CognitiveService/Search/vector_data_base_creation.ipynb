{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Azure Vector Data Base\n",
    "\n",
    "It is needed a vector data base and its integration with azure search service.\n",
    "\n",
    "* data is being vectorize with openAI ada embeddings model\n",
    "* from an existing json some fields are being vectorized for the search\n",
    "* the created index is uploaded to azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import json  \n",
    "import openai      # 1.3.3\n",
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "\n",
    "# Blob\n",
    "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "\n",
    "from azure.search.documents.indexes.models import (  \n",
    "    ExhaustiveKnnAlgorithmConfiguration,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    SemanticSearch,\n",
    "    VectorSearch,  \n",
    "    HnswAlgorithmConfiguration,\n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchProfile,\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    VectorSearch,\n",
    "    ExhaustiveKnnParameters,\n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticField,  \n",
    "    SearchField,  \n",
    "    VectorSearch,  \n",
    "    HnswParameters,  \n",
    "    VectorSearch,\n",
    "    VectorSearchAlgorithmKind,\n",
    "    VectorSearchAlgorithmMetric,\n",
    "    VectorSearchProfile,\n",
    ")  \n",
    "\n",
    "from dotenv import load_dotenv  \n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"pfd_conf.env\")\n",
    "\n",
    "connect_str = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "service_endpoint = os.getenv(\"AZURE_STORAGE_SERVICE_ENDPOINT\")\n",
    "\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "\n",
    "model = os.getenv(\"AZURE_EMBEDDINGS_MODEL\")\n",
    "credential = AzureKeyCredential(key)\n",
    "endpoint = os.getenv(\"AZUREOPENAI_ENDPOINT\")\n",
    "deployment = os.getenv(\"AZUREOPENAI_DEPLOYMENT\")\n",
    "\n",
    "\n",
    "container_name = \"containername\"  \n",
    "blob_name = 'original.json'  \n",
    "blob_vectorname = 'combinedVector.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_embeddings = AzureOpenAI(\n",
    "    api_key = os.getenv(\"OPENAI_EMBEDDINGS_KEY\"),  \n",
    "    api_version = os.getenv(\"OPENAI_EMBEDDINGS_VERSION\"),\n",
    "    azure_endpoint = os.getenv(\"OPENAI_EMBEDDINGS_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manage azure blob storage \n",
    "\n",
    "def update_json_blob(data, name_blob):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    blob_data = json.dumps(data, ensure_ascii=False).encode('utf-8')\n",
    "\n",
    "    blob_client = container_client.get_blob_client(name_blob)\n",
    "    blob_client.upload_blob(blob_data, overwrite=True, content_settings=ContentSettings(content_type='application/json'))\n",
    "\n",
    "\n",
    "def read_json_blob(name_blob):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(name_blob)\n",
    "\n",
    "    try:\n",
    "        downloaded_blob = blob_client.download_blob()\n",
    "        data = json.loads(downloaded_blob.readall().decode('utf-8'))\n",
    "    except:\n",
    "        data = []\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for a given text\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def generate_embeddings(text, model=model):\n",
    "        return client_embeddings.embeddings.create(input = [text], model=model).data[0].embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings Fields\n",
    "\n",
    "This would be generated from an already existing JSON file wich contains:\n",
    "\n",
    "* filename of the file\n",
    "* Conent of the file\n",
    "* folder of the file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = read_json_blob(blob_vectorname)\n",
    "input_data = read_json_blob(blob_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(input_data):\n",
    "    try:\n",
    "        id = item['id']\n",
    "        area = item['Area']\n",
    "        content = item['content']\n",
    "        content = ''.join(content)\n",
    "        item['content'] = content\n",
    "        filename = item['filename']  \n",
    "        title_embeddings = generate_embeddings(area, model)\n",
    "        item['AreaVector'] = title_embeddings\n",
    "        title_embeddings = generate_embeddings(content, model)\n",
    "        item['contentVector'] = title_embeddings\n",
    "        title_embeddings = generate_embeddings(filename, model)\n",
    "        item['filenameVector'] = title_embeddings\n",
    "        \n",
    "    except:\n",
    "        print(f\"Failed: {item['filename']}\"  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_json_blob(input_data, blob_vectorname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of an the vectorized index on azure search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "            SearchableField(name=\"Area\", type=SearchFieldDataType.String, filterable=True, facetable=True, sortable= True),\n",
    "            SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"filename\", type=SearchFieldDataType.String, filterable=True, facetable=True, sortable= True),\n",
    "            SearchField(name=\"AreaVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "            SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "            SearchField(name=\"filenameVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True, vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),\n",
    "        ]\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithms=[\n",
    "        HnswAlgorithmConfiguration(\n",
    "            name=\"myHnsw\", \n",
    "            #name=\"vector-config-1702077576158\",\n",
    "            kind=VectorSearchAlgorithmKind.HNSW,\n",
    "            parameters=HnswParameters(\n",
    "                m=10,\n",
    "                ef_construction=1000,\n",
    "                ef_search=900,\n",
    "                metric=VectorSearchAlgorithmMetric.COSINE\n",
    "            )\n",
    "        ),\n",
    "        ExhaustiveKnnAlgorithmConfiguration(\n",
    "            name=\"myExhaustiveKnn\",\n",
    "            kind=VectorSearchAlgorithmKind.EXHAUSTIVE_KNN,\n",
    "            parameters=ExhaustiveKnnParameters(\n",
    "                metric=VectorSearchAlgorithmMetric.DOT_PRODUCT\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    profiles=[\n",
    "        VectorSearchProfile(\n",
    "            name=\"myHnswProfile\",\n",
    "            algorithm_configuration_name=\"myHnsw\",\n",
    "            #algorithm_configuration_name=\"vector-config-1702077576158\",\n",
    "        ),\n",
    "        VectorSearchProfile(\n",
    "            name=\"myExhaustiveKnnProfile\",\n",
    "            algorithm_configuration_name=\"myExhaustiveKnn\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=SemanticPrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"Area\"),\n",
    "            content_fields=[SemanticField(field_name=\"content\"),\n",
    "            SemanticField(field_name=\"filename\"),]\n",
    "        )\n",
    ")\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ghcvindex created\n"
     ]
    }
   ],
   "source": [
    "index = SearchIndex(name=index_name, fields=fields,\n",
    "                    vector_search=vector_search, semantic_search=semantic_search)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading the index to azure search services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = read_json_blob(blob_vectorname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id must be  a string\n",
    "\n",
    "for item in documents:\n",
    "    item['id'] = str(item['id']) if 'id' in item else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index upload\n",
    "\n",
    "# search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "#result = search_client.upload_documents(documents)\n",
    "#print(f\"Uploaded {len(documents)} documents\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import AzureError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 100 documents out of 4855\n",
      "Uploaded 200 documents out of 4855\n",
      "Uploaded 300 documents out of 4855\n",
      "Uploaded 400 documents out of 4855\n",
      "Uploaded 500 documents out of 4855\n",
      "Uploaded 600 documents out of 4855\n",
      "Uploaded 700 documents out of 4855\n",
      "Uploaded 800 documents out of 4855\n",
      "Uploaded 900 documents out of 4855\n",
      "Uploaded 1000 documents out of 4855\n",
      "Uploaded 1100 documents out of 4855\n",
      "Uploaded 1200 documents out of 4855\n",
      "Uploaded 1300 documents out of 4855\n",
      "Uploaded 1400 documents out of 4855\n",
      "Uploaded 1500 documents out of 4855\n",
      "Uploaded 1600 documents out of 4855\n",
      "Uploaded 1700 documents out of 4855\n",
      "Uploaded 1800 documents out of 4855\n",
      "Uploaded 1900 documents out of 4855\n",
      "Uploaded 2000 documents out of 4855\n",
      "Uploaded 2100 documents out of 4855\n",
      "Uploaded 2200 documents out of 4855\n",
      "Uploaded 2300 documents out of 4855\n",
      "Uploaded 2400 documents out of 4855\n",
      "Uploaded 2500 documents out of 4855\n",
      "Uploaded 2600 documents out of 4855\n",
      "Uploaded 2700 documents out of 4855\n",
      "Uploaded 2800 documents out of 4855\n",
      "Uploaded 2900 documents out of 4855\n",
      "Uploaded 3000 documents out of 4855\n",
      "Uploaded 3100 documents out of 4855\n",
      "Uploaded 3200 documents out of 4855\n",
      "Uploaded 3300 documents out of 4855\n",
      "Uploaded 3400 documents out of 4855\n",
      "Uploaded 3500 documents out of 4855\n",
      "Uploaded 3600 documents out of 4855\n",
      "Uploaded 3700 documents out of 4855\n",
      "Uploaded 3800 documents out of 4855\n",
      "Uploaded 3900 documents out of 4855\n",
      "Uploaded 4000 documents out of 4855\n",
      "Uploaded 4100 documents out of 4855\n",
      "Uploaded 4200 documents out of 4855\n",
      "Uploaded 4300 documents out of 4855\n",
      "Uploaded 4400 documents out of 4855\n",
      "Uploaded 4500 documents out of 4855\n",
      "Uploaded 4600 documents out of 4855\n",
      "Uploaded 4700 documents out of 4855\n",
      "Uploaded 4800 documents out of 4855\n",
      "Uploaded 4855 documents out of 4855\n"
     ]
    }
   ],
   "source": [
    "# Index upload by batch\n",
    "\n",
    "batch_size = 100  # Adjust the batch size according to your needs\n",
    "\n",
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "total_documents = len(documents)\n",
    "uploaded_documents = 0\n",
    "\n",
    "while uploaded_documents < total_documents:\n",
    "    batch = documents[uploaded_documents:uploaded_documents + batch_size]\n",
    "    try:\n",
    "        result = search_client.upload_documents(batch)\n",
    "        uploaded_documents += len(batch)\n",
    "        print(f\"Uploaded {uploaded_documents} documents out of {total_documents}\")\n",
    "    except AzureError as e:\n",
    "        if \"Request Entity Too Large\" in str(e):\n",
    "            # If the batch is too large, decrease the batch size and try again\n",
    "            \n",
    "            batch_size //= 2\n",
    "            print(f\"Batch size reduced to {batch_size} due to 'RequestEntityTooLargeError'\")\n",
    "        else:\n",
    "            # Handle other Azure errors here\n",
    "            print(f\"An Azure error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
